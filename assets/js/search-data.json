{
  
    
        "post0": {
            "title": "AllCorrect DS Project",
            "content": "Task . Build an algorithm for classification of user reviews into one of the four categories. The quality of the algorithm should be evaluated using hold-out subset or crossvalidation technique. . Plan . Data Preprocessing | EDA | Baseline model + TFDIF Train and test split | Vectorization | Training | . | CatBoost | BERT embedding | Keras + BERT | Conclusion | Python script | Let&#39;s import all needed libs . import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers . import torch import transformers from tqdm.auto import tqdm . import pandas as pd import numpy as np import re import math from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score from sklearn.feature_extraction.text import TfidfVectorizer from nltk.tokenize import word_tokenize from nltk.stem.wordnet import WordNetLemmatizer from nltk.corpus import stopwords from catboost import CatBoostClassifier, Pool import joblib . df = pd.read_excel(path) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51800 entries, 0 to 51799 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 id 51800 non-null int64 1 mark 51800 non-null object 2 review 51800 non-null object dtypes: int64(1), object(2) memory usage: 1.2+ MB . df.iloc[0] . id 6720 mark RL review It&#39;s not Turkish, it&#39;s a lie, but I recommend ... Name: 0, dtype: object . Preprocessing . lemmatizer = WordNetLemmatizer() def clean_text(text): #lower text = text.strip().lower() #clear the text text = re.sub(r&quot;[^a-zA-Z&#39;]+&quot;, &#39; &#39;, text) #lemmatization tokens = word_tokenize(text) lemmas = [lemmatizer.lemmatize(word) for word in tokens] return &quot; &quot;.join(lemmas) df[&#39;review&#39;] = df[&#39;review&#39;].apply(clean_text) . df[&#39;review_len&#39;] = df[&#39;review&#39;].str.len() df = df.sort_values(&#39;review_len&#39;,axis=0) df.head() . id mark review review_len . 8779 47782 | YL | | 0 | . 48906 24112 | RL | | 0 | . 45267 7454 | RL | | 0 | . 26259 27730 | RL | | 0 | . 14029 47768 | RL | | 0 | . df[df[&#39;review_len&#39;] == 0][&#39;review_len&#39;].count() . 14 . df = df[df[&#39;review_len&#39;] != 0] . df[&#39;mark&#39;] = df[&#39;mark&#39;].str.upper() df[&#39;mark&#39;].unique() . array([&#39;RL&#39;, &#39;L-&#39;, &#39;L+&#39;, &#39;YL&#39;], dtype=object) . encoder = LabelEncoder() df[&#39;mark_num&#39;] = encoder.fit_transform(df[&#39;mark&#39;]) . df[&#39;mark_num&#39;].unique() . array([2, 1, 0, 3]) . EDA . df[&#39;mark&#39;].value_counts(normalize=True).plot(kind=&#39;pie&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4381c53a50&gt; . df[(df[&#39;review_len&#39;] &lt; 2000) &amp; (df[&#39;review_len&#39;] &gt; 250)][&#39;review_len&#39;].hist(bins=100,density=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4381a85110&gt; . The data is unbalanced. There is 82% of the one type out of four. . Most of reviews are not longer than 500 signs. . Baseline Model + TFIDF . Train and test split . stop_words = set(stopwords.words(&#39;english&#39;)) corpus = df[&#39;review&#39;] . X = df[&#39;review&#39;] y = df[&#39;mark_num&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) . Vectorization . count_tf_idf = TfidfVectorizer(stop_words = stop_words) tf_idf_train = count_tf_idf.fit_transform(X_train) tf_idf_test = count_tf_idf.transform(X_test) . Training . lr_model = LogisticRegression(multi_class=&#39;ovr&#39;, solver=&#39;liblinear&#39;) lr_model.fit(tf_idf_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False) . y_pred = lr_model.predict(tf_idf_test) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.75 0.02 0.04 133 1 0.80 0.68 0.73 1096 2 0.91 0.99 0.94 8537 3 0.70 0.18 0.29 592 accuracy 0.89 10358 macro avg 0.79 0.47 0.50 10358 weighted avg 0.88 0.89 0.87 10358 . print(confusion_matrix(y_test, y_pred)) . [[ 3 45 70 15] [ 1 742 350 3] [ 0 97 8412 28] [ 0 42 443 107]] . print(f1_score(y_test, y_pred, average=&#39;weighted&#39;)) . 0.8731152130914144 . print(accuracy_score(y_test, y_pred)) . 0.8943811546630623 . CatBoost . X = df[[&#39;review&#39;, &#39;id&#39;]] y = df[&#39;mark_num&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) . train_pool = Pool(data=X_train, label=y_train, text_features=[&#39;review&#39;]) valid_pool = Pool(data=X_test, label=y_test, text_features=[&#39;review&#39;]) model_CB = CatBoostClassifier(loss_function=&#39;MultiClass&#39;) . # joblib.dump(model_CB, &#39;/content/drive/My Drive/data/model_CB.joblib&#39;) model_CB = joblib.load(&#39;/content/drive/My Drive/data/model_CB.joblib&#39;) y_pred = model_CB.predict(X_test) # print(model_CB.get_best_score()) . print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.69 0.23 0.35 133 1 0.82 0.78 0.80 1096 2 0.94 0.98 0.96 8537 3 0.79 0.43 0.56 592 accuracy 0.92 10358 macro avg 0.81 0.61 0.67 10358 weighted avg 0.91 0.92 0.91 10358 . print(f1_score(y_test, y_pred, average=&#39;weighted&#39;)) . 0.9115470087472277 . print(accuracy_score(y_test, y_pred)) . 0.9196756130527128 . BERT embedding . bert_tokenizer = transformers.BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) . bert_model = transformers.BertModel.from_pretrained(&#39;bert-base-uncased&#39;) . Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: [&#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). . . def BERT_process(texts, max_length=512, batch_size=100, disable_progress_bar=True): ids_list = [] attention_mask_list = [] # text to padded IDs of tokens along with their attention masks for input_text in tqdm(texts, disable=disable_progress_bar): ids = bert_tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length) padded = np.array(ids + [0]*(max_length - len(ids))) attention_mask = np.where(padded != 0, 1, 0) ids_list.append(padded) attention_mask_list.append(attention_mask) # use cuda if possible: device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) bert_model.to(device) # gettings embeddings in batches embeddings = [] for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar): ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device) attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device) with torch.no_grad(): bert_model.eval() batch_embeddings = bert_model(input_ids=ids_batch, attention_mask=attention_mask_batch) embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy()) return np.concatenate(embeddings) . Keras + BERT . # np.savez_compressed(&#39;/content/drive/My Drive/data/X_BERT.npz&#39;, X=X) with np.load(&#39;/content/drive/My Drive/data/X_BERT.npz&#39;) as data: X = data[&#39;X&#39;] . y = df[&#39;mark_num&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=.5) . y_train = np.asarray(y_train).astype(np.float32) train_dataset = tf.data.Dataset.from_tensor_slices(( X_train, y_train )) test_dataset = tf.data.Dataset.from_tensor_slices(( X_test, y_test )) . model = keras.models.Sequential() optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) # optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) model.add(tf.keras.layers.Dense(16, activation=&quot;relu&quot;)) model.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) model.add(tf.keras.layers.Dense(128, activation=&#39;softmax&#39;)) # model.compile(optimizer=optimizer, loss=&#39;sparse_categorical_crossentropy&#39;) model.compile( optimizer=optimizer, # Optimizer # Loss function to minimize loss=keras.losses.SparseCategoricalCrossentropy(), # List of metrics to monitor metrics=[keras.metrics.SparseCategoricalAccuracy()], ) model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16, validation_data=(X_val, y_val)) y_pred = model.predict(X_test, verbose=True).argmax(axis=-1) . Epoch 1/3 2266/2266 [==============================] - 6s 2ms/step - loss: 0.4185 - sparse_categorical_accuracy: 0.8592 - val_loss: 0.3678 - val_sparse_categorical_accuracy: 0.8704 Epoch 2/3 2266/2266 [==============================] - 5s 2ms/step - loss: 0.3630 - sparse_categorical_accuracy: 0.8734 - val_loss: 0.3487 - val_sparse_categorical_accuracy: 0.8791 Epoch 3/3 2266/2266 [==============================] - 5s 2ms/step - loss: 0.3513 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8808 243/243 [==============================] - 0s 934us/step . . print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.29 0.06 0.11 108 1 0.63 0.72 0.67 786 2 0.92 0.96 0.94 6427 3 0.63 0.21 0.31 447 accuracy 0.88 7768 macro avg 0.62 0.49 0.51 7768 weighted avg 0.86 0.88 0.86 7768 . print(f1_score(y_test, y_pred, average=&#39;weighted&#39;)) . 0.8615621529186347 . print(accuracy_score(y_test, y_pred)) . 0.8761585993820803 . Conclusion . The score of deep-learning model + BERT is the lest than accuracy of a simple LogisticRegression model. . Probably it&#39;s because the data is unbalanced. If the data are a bit more balanced, perhaps we would have a better result. . I decided to continue with already trained Catboost model, that showed the best result, and create python script for production. . Python script . Code of the python script to classify users&#39; reviews. . Input: . CatBoost model | Excel file with two columns. First column is unique identifier of the review, second column is review&#39;s text. | . Output: . Excel file with six columns. First two columns remain from the input file, other columns contain probability to belong to each category. | . import nltk nltk.download(&#39;punkt&#39;) nltk.download(&#39;wordnet&#39;) nltk.download(&#39;stopwords&#39;) from nltk.tokenize import word_tokenize from nltk.stem.wordnet import WordNetLemmatizer from nltk.corpus import stopwords import joblib import pandas as pd from optparse import OptionParser import re parser = OptionParser() parser.add_option(&quot;-t&quot;, &quot;--text_file&quot;, action=&quot;store&quot;, type=&quot;string&quot;, dest=&quot;text_path&quot;) parser.add_option(&quot;-m&quot;, &quot;--model_file&quot;, action=&quot;store&quot;, type=&quot;string&quot;, dest=&quot;model_path&quot;) (options, args) = parser.parse_args() lemmatizer = WordNetLemmatizer() def clean_text(text): #lower text = text.strip().lower() #clear the text text = re.sub(r&quot;[^a-zA-Z&#39;]+&quot;, &#39; &#39;, text) #lemmatization tokens = word_tokenize(text) lemmas = [lemmatizer.lemmatize(word) for word in tokens] return &quot; &quot;.join(lemmas) def predict_mark(text): model = joblib.load(model_path) y_pred = model.predict(text_preprocessed) return y_pred df = pd.read_excel(text_path) df[&#39;review&#39;] = df[&#39;review&#39;].apply(clean_text) df[&#39;mark&#39;] = df[&#39;review&#39;].apply(predict_mark) df.to_excel(&quot;output.xlsx&quot;) .",
            "url": "https://pluzharovakp.github.io/ds_projects/2021/06/28/AllCorrect-DS-Project.html",
            "relUrl": "/2021/06/28/AllCorrect-DS-Project.html",
            "date": " â€¢ Jun 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "AllCorrect DA Project",
            "content": "Plan . Data Preprocessing | Select localization reviews | Detect language and translate the reviews Determine language | Translate the texts | | Sentiment Analysis | EDA Discover the score mean | Discover the reviews count | Top languages among localization reviews | | Conclusion | Imports and preprocessing . df = pd.read_csv(&#39;/content/drive/My Drive/data/df.csv&#39;, index_col=0) . df.columns = [&#39;id&#39;, &#39;score&#39;, &#39;text&#39;] . df.shape . (1869775, 3) . df.head() . id score text . 0 title777 | 4 | Velmi nÃ¡vykovÃ¡ hra, skvÄ›le zpracovanÃ¡. Jen Å¡ko... | . 1 title777 | 4 | Hra hezkÃ¡, vadÃ­ mi, Å¾e tÅ™eba pÅ™es noc se nepln... | . 2 title777 | 5 | Cool | . 3 title777 | 5 | SuprovÃ¡ hra. | . 4 title777 | 5 | âš½ï¸ğŸ€ğŸˆâš¾ï¸ğŸ¥å»æµè§ˆå™¨æ‰“å¼€ğŸ“²87á¹šĞ¼ï¼¢,Câ˜»Ğ¼ğŸ“²åäººæœ€å¤§å¨±ä¹ç«™ï¼Œå¼€æˆ·å…è´¹é€ç°é‡‘100å—â€¼ï¸-... | . df = df.dropna() . df = df.drop_duplicates().reset_index(drop=True) . Select localization reviews . keywords = pd.read_excel(&#39;/content/drive/My Drive/data/Keywords.xlsx&#39;) . keywords . Language combination &quot;Translation&quot; &quot;Language&quot; &quot;Localization&quot; &quot;English&quot; Language Unnamed: 6 ISO 639-1 ISO 3166-1-alpha-2 Unnamed: 9 . 0 English - French | Traduction | la langue | localisation | anglais | FranÃ§ais | NaN | FR | FR | NaN | . 1 English - German | Ãœbersetzung | Sprache | Lokalisierung | Englisch | Deutsch | NaN | DE | DE | NaN | . 2 English - Italian | traduzione | lingua | localizzazione | inglese | Italiano | NaN | IT | IT | NaN | . 3 English - Spanish | traducciÃ³n | idioma | localizaciÃ³n | inglÃ©s | espaÃ±ol | NaN | ES | ES | NaN | . 4 English - Chinese Simplified | ç¿»è¯‘ | è¯­è¨€ | æœ¬åœŸåŒ– | è‹±è¯­ | ä¸­å›½ | NaN | ZH | CN | NaN | . 5 English - Chinese Traditional | ç¿»è­¯ | èªè¨€ | æœ¬åœŸåŒ– | è‹±èª | ä¸­åœ‹ | NaN | ZH | CN | NaN | . 6 English - Japanese | ç¿»è¨³ | è¨€èª | å±€åœ°åŒ– | è‹±èª | æ—¥æœ¬ | NaN | JA | JP | NaN | . 7 English - Korean | ë²ˆì—­ | ì–¸ì–´ | ì§€ë°©í™” | ì˜ì–´ | í•œêµ­ | NaN | KO | KR | NaN | . 8 English - Portuguese (BRZ) | traduÃ§Ã£o | lÃ­ngua | localizaÃ§Ã£o | inglÃªs | Portugues | NaN | PT | BR | NaN | . 9 English - Portuguese (EUR) | traduÃ§Ã£o | lÃ­ngua | localizaÃ§Ã£o | inglÃªs | Portugues | NaN | PT | PT | NaN | . 10 English - Polish | tÅ‚umaczenie | jÄ™zyk | Lokalizacja | angielski | polski | NaN | PL | PL | NaN | . 11 English - Turkish | Ã§eviri | dil | yerelleÅŸtirme | Ä°ngilizce | TÃ¼rk | NaN | TR | TR | NaN | . 12 English - Arabic | ØªØ±Ø¬Ù…Ø© | Ù„ØºØ© | Ø§Ù„Ù…ÙˆÙ‚Ø¹ | Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© | NaN | AR | EG, SA, AE | NaN | . 13 English - Russian | ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ | ÑĞ·Ñ‹Ğº | Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ | Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ | Ñ€ÑƒÑÑĞºĞ¸Ğ¹ | NaN | RU | RU, BY, KZ, KG | NaN | . 14 English - Dutch | vertaling | taal | lokalisatie | Engels | Nederlands | NaN | NL | NL | NaN | . 15 English - Swedish | Ã¶versÃ¤ttning | sprÃ¥k | lokalisering | engelska | svenska | NaN | SV | SE | NaN | . 16 English - Danish | oversÃ¦ttelse | Sprog | lokalisering | engelsk | dansk | NaN | DA | DK | NaN | . 17 English - Czech | pÅ™eklad | Jazyk | lokalizace | AnglickÃ½ | ÄŒesky | NaN | CS | CZ | NaN | . 18 English - Finnish | kÃ¤Ã¤nnÃ¶s | Kieli | lokalisointi | englanti | suomalainen | NaN | FI | FI | NaN | . 19 English - Greek | Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ· | Î“Î»ÏÏƒÏƒÎ± | ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ | Î±Î³Î³Î»Î¹ÎºÎ¬ | Î•Î»Î»Î·Î½Î¹ÎºÎ¬ | NaN | EL | GR | NaN | . 20 English - Norwegian | oversettelse | SprÃ¥k | lokalisering | engelsk | norsk | NaN | NO | NO | NaN | . 21 English - Icelandic | Ã¾Ã½Ã°ing | tungumÃ¡l | staÃ°setning | ensku | Ãslensku | NaN | IS | IS | NaN | . 22 English - Vietnamese | dá»‹ch | ngÃ´n ngá»¯ | ná»™i Ä‘á»‹a hoÃ¡ | tiáº¿ng Anh | Tiáº¿ng Viá»‡t | NaN | VI | VN | NaN | . 23 English - Hindi | à¤…à¤¨à¥à¤µà¤¾à¤¦ | à¤­à¤¾à¤·à¤¾ | à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯à¤•à¤°à¤£ | à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¥€ | à¤¹à¤¿à¤¨à¥à¤¦à¥€ | NaN | HI | IN | NaN | . 24 English - Indonesian | terjemahan | bahasa | penyetempatan | bahasa inggris | indonesia | NaN | ID | ID | NaN | . 25 English - Thai | à¸à¸²à¸£à¹à¸›à¸¥ | à¸ à¸²à¸©à¸² | à¸à¸²à¸£ à¸ˆà¸³à¸à¸±à¸” | à¸­à¸±à¸‡à¸à¸¤à¸© | à¹„à¸—à¸¢ | NaN | TH | TH | NaN | . 26 English - Malayan | terjemahan | bahasa | penyetempatan | bahasa inggeris | Melayu | NaN | MS | MY, SG | NaN | . 27 English - Hebrew | ×ªÖ´×¨×’×•Ö¼× | ×©×‚Ö¸×¤Ö¸×” | ×œ×•×§×œ×™×–×¦×™×” | ×× ×’×œ×™×ª | ×¢×‘×¨×™×ª | NaN | HE | IL | NaN | . 28 English - English | translation | language | localization | english | english | NaN | EN | US, CA, AU, GB, IE, NZ | NaN | . 29 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . . keywords = keywords.drop(29, axis=0) . keywords[&#39;Language combination&#39;] = keywords[&#39;Language combination&#39;].str.replace(&#39;English - &#39;, &#39;&#39;) . keywords_list = keywords[&#39;&quot;Translation&quot;&#39;].tolist() + keywords[&#39;&quot;Language&quot;&#39;].tolist() + keywords[&#39;&quot;Localization&quot;&#39;].tolist() + keywords[&#39;&quot;English&quot;&#39;].tolist() + keywords[&#39;Language&#39;].tolist() + keywords[&#39;Language combination&#39;].tolist() . from nltk.stem.porter import PorterStemmer stemmer = PorterStemmer() stemmer_rus = SnowballStemmer(&#39;russian&#39;) keywords_stem = [stemmer.stem(word) for word in keywords_list] . keywords_stem = keywords_stem + [&#39;ç¿»è¯‘&#39;, &#39;è¯­è¨€&#39;, &#39;ä¸­æ–‡&#39;, &#39;ä¸­å›½&#39;, &#39;æ±‰è¯­&#39;, &#39;æœ¬åœŸåŒ–&#39;, &#39;è‹±è¯­&#39;, &#39;ç‰ˆæœ¬&#39;, &#39;åŒºåŸŸ&#39;, &#39;è·¨åœ°åŒº&#39;, &#39;åœ°åŒº&#39;, &#39;åè¯­&#39;, &#39;æ±‰åŒ–&#39;, &#39;è‹±æ–‡&#39;, &#39;å›½å®¶&#39;, &#39;å›½æœ&#39;, &#39;VPN&#39;, &#39;Ñ€ÑƒÑÑ&#39;, &#39;Ğ°Ğ½Ğ³Ğ»&#39;] . keywords_stem = [keywords_stem[i].replace(&#39; &#39;, &#39;&#39;) for i in range(len(keywords_stem))] . keywords_string = &#39;|&#39;.join(keywords_stem) . df[&#39;is_about_localization&#39;] = df[&#39;text&#39;].str.lower().str.contains(keywords_string.lower()) . df.head() . id score text is_about_localization . 0 title777 | 4 | Velmi nÃ¡vykovÃ¡ hra, skvÄ›le zpracovanÃ¡. Jen Å¡ko... | False | . 1 title777 | 4 | Hra hezkÃ¡, vadÃ­ mi, Å¾e tÅ™eba pÅ™es noc se nepln... | False | . 2 title777 | 5 | Cool | False | . 3 title777 | 5 | SuprovÃ¡ hra. | False | . 4 title777 | 5 | âš½ï¸ğŸ€ğŸˆâš¾ï¸ğŸ¥å»æµè§ˆå™¨æ‰“å¼€ğŸ“²87á¹šĞ¼ï¼¢,Câ˜»Ğ¼ğŸ“²åäººæœ€å¤§å¨±ä¹ç«™ï¼Œå¼€æˆ·å…è´¹é€ç°é‡‘100å—â€¼ï¸-... | False | . df_loc = df[df[&#39;is_about_localization&#39;]==True] . df_loc.shape . (36031, 4) . df_loc . id score text is_about_localization . 8 title777 | 1 | æ²¡æœ‰åè¯­ç‰ˆæœ¬ | True | . 11 title777 | 5 | æ²¡æœ‰ä¸­æ–‡å­—.å¾ˆéš¾ç©-å¾ˆå¥½ç©çš„æ¸¸æˆ.åªå¯æƒœè¯­è¨€ä¸æ˜ | True | . 176 title777 | 5 | Excellent en franÃ§ais et connexion Facebook sa... | True | . 967 title777 | 5 | ì¬ë°Œì–´ìš”. ì˜ì–´ê¹Œë§‰ëˆˆì´ë¼ í•œêµ­ì–´ íŒ¨ì¹˜ ëìŒ ì¢‹ê² ì–´ìš”. | True | . 968 title777 | 5 | ì—°ë£Œ.. ë„ˆë¬´ì•ˆì£¼ê³ ã…œ í•œêµ­ì–´ë¡œì¢€ ë²ˆì—­ì¢€í•´ì£¼ì‹œì§€. .ì •ë³´ë„ì—†ê³ ã…œã…œ ì¼ìˆê¸´ ì¼ìˆìŒ ã…‹ã…‹ | True | . ... ... | ... | ... | ... | . 1560168 title642 | 5 | HARÄ°KA YOL YAPMA VE ARABALAR HARÄ°KA ONDAN 5 YI... | True | . 1560176 title642 | 5 | Oyun fanadil | True | . 1560184 title642 | 3 | gÃ¼zel gibi yol yapÄ±p arabalarÄ± geÃ§irmeye Ã§alÄ±ÅŸ... | True | . 1560757 title642 | 2 | I really like this game but they should update... | True | . 1563921 title642 | 5 | Like si viste el video de fernanflo +german ga... | True | . 36031 rows Ã— 4 columns . Detect language and translate the reviews . Determine language . def clear_text(text): new_text = re.sub(r&quot;[^a-zA-Z&#39;]&quot;, &quot; &quot;, text) new_text = new_text.split() new_text = &quot; &quot;.join(new_text) return new_text.lower() . . def determine_language(text): try: return langdetect.detect(text) except: return None . df_loc[&#39;language&#39;] = df_loc[&#39;text&#39;].apply(determine_language) . df_loc[&#39;language&#39;].unique() . array([&#39;zh-cn&#39;, &#39;fr&#39;, &#39;ko&#39;, &#39;ja&#39;, &#39;pt&#39;, &#39;pl&#39;, &#39;ru&#39;, &#39;en&#39;, &#39;tr&#39;, &#39;de&#39;, &#39;vi&#39;, &#39;it&#39;, &#39;no&#39;, &#39;th&#39;, &#39;es&#39;, &#39;hu&#39;, &#39;zh-tw&#39;, &#39;tl&#39;, &#39;af&#39;, &#39;et&#39;, &#39;cs&#39;, &#39;cy&#39;, &#39;hi&#39;, &#39;sl&#39;, &#39;so&#39;, &#39;sk&#39;, &#39;fi&#39;, &#39;ar&#39;, &#39;id&#39;, &#39;ca&#39;, &#39;nl&#39;, &#39;el&#39;, &#39;he&#39;, &#39;da&#39;, &#39;bg&#39;, &#39;mk&#39;, &#39;uk&#39;, &#39;sq&#39;, &#39;hr&#39;, &#39;sv&#39;, &#39;lv&#39;, &#39;lt&#39;, &#39;mr&#39;, &#39;ro&#39;, None, &#39;sw&#39;], dtype=object) . df_loc[df_loc[&#39;language&#39;].isnull()] . id score text is_about_localization language . 403379 title249 | 1 | RUSSIAN tanks/planes are the way to go........... | True | None | . df_loc[&#39;language&#39;] = df_loc[&#39;language&#39;].fillna(&#39;en&#39;) . df_loc.groupby(&#39;language&#39;)[&#39;language&#39;].count().sort_values() . language mr 1 lt 3 hi 3 lv 3 mk 3 sw 10 uk 11 hr 11 hu 12 sk 15 sq 15 cy 17 ro 17 bg 21 he 22 fi 29 da 31 sv 35 sl 38 ca 40 et 41 el 46 no 50 so 52 tl 83 af 99 vi 108 cs 120 zh-tw 167 nl 316 pl 772 ar 796 ja 804 es 884 it 1367 de 1405 fr 1498 id 1583 ko 1611 th 1731 ru 2579 zh-cn 3133 pt 3754 tr 4509 en 8186 Name: language, dtype: int64 . . df_loc[&#39;language&#39;] = df_loc[&#39;language&#39;].str.replace(&#39;zh-tw&#39;, &#39;zh&#39;) df_loc[&#39;language&#39;] = df_loc[&#39;language&#39;].str.replace(&#39;zh-cn&#39;, &#39;zh&#39;) . table_lang_and_codes = keywords[[&#39;Language combination&#39;, &#39;ISO 639-1&#39;]] . table_lang_and_codes[&#39;ISO 639-1&#39;] = table_lang_and_codes[&#39;ISO 639-1&#39;].str.lower() . table_lang_and_codes.columns = [&#39;language_full&#39;, &#39;language&#39;] . table_lang_and_codes[&#39;language&#39;] = table_lang_and_codes[&#39;language&#39;].drop_duplicates() . table_lang_and_codes = table_lang_and_codes.dropna() . table_lang_and_codes[&#39;language_full&#39;] = table_lang_and_codes[&#39;language_full&#39;].str.replace(&#39;Chinese Simplified&#39;, &#39;Chinese&#39;) . table_lang_and_codes[&#39;language_full&#39;] = table_lang_and_codes[&#39;language_full&#39;].where(table_lang_and_codes[&#39;language_full&#39;] != &#39;Portuguese (BRZ)&#39;, &#39;Portuguese&#39;) . table_lang_and_codes . language_full language . 0 French | fr | . 1 German | de | . 2 Italian | it | . 3 Spanish | es | . 4 Chinese | zh | . 6 Japanese | ja | . 7 Korean | ko | . 8 Portuguese | pt | . 10 Polish | pl | . 11 Turkish | tr | . 12 Arabic | ar | . 13 Russian | ru | . 14 Dutch | nl | . 15 Swedish | sv | . 16 Danish | da | . 17 Czech | cs | . 18 Finnish | fi | . 19 Greek | el | . 20 Norwegian | no | . 21 Icelandic | is | . 22 Vietnamese | vi | . 23 Hindi | hi | . 24 Indonesian | id | . 25 Thai | th | . 26 Malayan | ms | . 27 Hebrew | he | . 28 English | en | . . df_loc_new = df_loc.merge(table_lang_and_codes, on=&#39;language&#39;, how=&#39;left&#39;) . df_loc_new.head() . id score text is_about_localization language language_full . 0 title777 | 1 | æ²¡æœ‰åè¯­ç‰ˆæœ¬ | True | zh | Chinese | . 1 title777 | 5 | æ²¡æœ‰ä¸­æ–‡å­—.å¾ˆéš¾ç©-å¾ˆå¥½ç©çš„æ¸¸æˆ.åªå¯æƒœè¯­è¨€ä¸æ˜ | True | zh | Chinese | . 2 title777 | 5 | Excellent en franÃ§ais et connexion Facebook sa... | True | fr | French | . 3 title777 | 5 | ì¬ë°Œì–´ìš”. ì˜ì–´ê¹Œë§‰ëˆˆì´ë¼ í•œêµ­ì–´ íŒ¨ì¹˜ ëìŒ ì¢‹ê² ì–´ìš”. | True | ko | Korean | . 4 title777 | 5 | ì—°ë£Œ.. ë„ˆë¬´ì•ˆì£¼ê³ ã…œ í•œêµ­ì–´ë¡œì¢€ ë²ˆì—­ì¢€í•´ì£¼ì‹œì§€. .ì •ë³´ë„ì—†ê³ ã…œã…œ ì¼ìˆê¸´ ì¼ìˆìŒ ã…‹ã…‹ | True | ko | Korean | . ... ... | ... | ... | ... | ... | ... | . 36026 title642 | 5 | HARÄ°KA YOL YAPMA VE ARABALAR HARÄ°KA ONDAN 5 YI... | True | en | English | . 36027 title642 | 5 | Oyun fanadil | True | tr | Turkish | . 36028 title642 | 3 | gÃ¼zel gibi yol yapÄ±p arabalarÄ± geÃ§irmeye Ã§alÄ±ÅŸ... | True | tr | Turkish | . 36029 title642 | 2 | I really like this game but they should update... | True | en | English | . 36030 title642 | 5 | Like si viste el video de fernanflo +german ga... | True | it | Italian | . 36031 rows Ã— 6 columns . df_loc_new[df_loc_new[&#39;language_full&#39;].isnull()].groupby(&#39;language&#39;)[&#39;language&#39;].count() . language af 99 bg 21 ca 40 cy 17 et 41 hr 11 hu 12 lt 3 lv 3 mk 3 mr 1 ro 17 sk 15 sl 38 so 52 sq 15 sw 10 tl 83 uk 11 Name: language, dtype: int64 . There are some reviews on languages we didn&#39;t have in keywords lists, but they include words are common with languages we do have in keywords. We can translate them to english to process them futher. . df_loc_known = df_loc_new.dropna() . df_loc_known.shape . (35539, 6) . Translate the texts . # translator = GoogleTranslator(source=&#39;auto&#39;, target=&#39;en&#39;) # def translate_to_en(text): # try: # return translator.translate(text) # except: # return None # df_loc_known[&#39;en_trans&#39;] = df_loc_known[&#39;text&#39;].apply(translate_to_en) . . df_loc_known = pd.read_csv(&#39;/content/drive/My Drive/data/df_loc_known.csv&#39;, index_col=0) . df_loc_known[df_loc_known[&#39;en_trans&#39;].isnull()][&#39;language&#39;].count() . 343 . We have only 343 reviews without translation. They can be used for test our future model. . df_final = df_loc_known[df_loc_known[&#39;en_trans&#39;].isnull()==False] . Sentiment Analysis . nltk.download(&#39;vader_lexicon&#39;) . [nltk_data] Downloading package vader_lexicon to /root/nltk_data... . True . from nltk.sentiment.vader import SentimentIntensityAnalyzer . sid = SentimentIntensityAnalyzer() df_final[&#39;sentiment_vader&#39;] = df_final[&#39;en_trans&#39;].apply(lambda text: sid.polarity_scores(text)) . for i in range(20,30): print(&#39;Row &#39;, i) print(df_final.loc[i, &#39;en_trans&#39;]) print(df_final.loc[i, &#39;sentiment_vader&#39;][&#39;compound&#39;]) print(&#39;&#39;) . Row 20 I&#39;m from Cooking Fever. If you translate it into Japanese, it will be 5 stars without complaint. 0.2235 Row 21 Thank you for your support in Japanese! 0.6696 Row 22 I hope you can make it in Japanese 0.4404 Row 23 In Japanese please! 0.3802 Row 24 Make it Japanese! I don&#39;t know what it means! 0.0 Row 25 Very addictive! I loved. I just thought the delay in filling the fuel was bad and little chance of winning diamonds. They did not put the language in Portuguese. Improve this please, the game has everything to be a success â¤ 0.9142 Row 26 The game is sensational but could have the option of Portuguese 0.0 Row 27 No Polish language -0.296 Row 28 It&#39;s hard to get at least 1 star without diamond products, especially above level 30. The lack of the Polish language does not bother me. Interesting but not easy game. -0.6796 Row 29 It is a pity that the menu is not in Polish, but the overall sound is great 0.7227 . . df_final[&#39;compound&#39;] = df_final[&#39;en_trans&#39;].apply(lambda text: sid.polarity_scores(text)[&#39;compound&#39;]) . df_final[&#39;score_sentiment&#39;] = pd.cut(df_final[&#39;compound&#39;], 5, labels=[1, 2, 3, 4, 5]).astype(int) . Compound is a balanced valuation of a sentiment, that takes in account both negative and posive values. That is why it&#39;s used for further analysis. . EDA . Discover the score mean . df_final[&#39;is_pos&#39;] = (df_final[&#39;compound&#39;] &gt;= 0).astype(int) . games_with_zero = df_final[(df_final[&#39;score&#39;] &lt; 1) &amp; (df_final[&#39;id&#39;].count() &gt; 1)][&#39;id&#39;].unique().tolist() . for i in games_with_zero: print(i, &#39;Scores:&#39;, df_final[df_final[&#39;id&#39;] == i][&#39;score&#39;].unique(), &#39;Number of reviews:&#39;, df_final[df_final[&#39;id&#39;] == i][&#39;id&#39;].count()) print(&#39;&#39;) . All games that have 0 as a score have score 1 in addition. Probably there was kind of &quot;do you like/ not like our game&quot; question in a game. That&#39;s why there are 22 games with only 2 scores. . avrg_score = pd.DataFrame(df_final.groupby(&#39;id&#39;)[&#39;score&#39;].mean()) avrg_score_sentiment = pd.DataFrame(df_final.groupby(&#39;id&#39;)[&#39;score_sentiment&#39;].mean()) avrg_compound_sentiment = pd.DataFrame(df_final.groupby(&#39;id&#39;)[&#39;is_pos&#39;].mean()) . scores_all = avrg_score[(~avrg_score.index.isin(games_with_zero)) &amp; (df_final.groupby(&#39;id&#39;)[&#39;score&#39;].count() &gt; 1)].merge(avrg_score_sentiment, on=&#39;id&#39;) . scores_all = scores_all.round(3) . scores_all.sort_values(by=&#39;score_sentiment&#39;).head() . score score_sentiment . id . title182 1.500 | 2.500 | . title697 4.667 | 2.833 | . title207 2.000 | 3.000 | . title83 1.000 | 3.000 | . title31 3.750 | 3.000 | . compound_all = avrg_score[avrg_score.index.isin(games_with_zero)].merge(avrg_compound_sentiment, on=&#39;id&#39;) . compound_all = compound_all.round(3) . compound_all.sort_values(by=&#39;is_pos&#39;).head() . score is_pos . id . title451 0.889 | 0.540 | . title249 0.554 | 0.603 | . title508 0.843 | 0.604 | . title81 0.771 | 0.693 | . title417 0.685 | 0.697 | . scores_all[:50].plot.bar(figsize=(30, 10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8952464590&gt; . review_count = df_final[~df_final[&#39;id&#39;].isin(games_with_zero)].groupby(&#39;id&#39;)[&#39;text&#39;].count().reset_index() . review_count.columns = [&#39;id&#39;, &#39;text_count&#39;] . review_count.head() . id text_count . 0 title101 | 171 | . 1 title114 | 5 | . 2 title13 | 4 | . 3 title143 | 4 | . 4 title146 | 43 | . scores_all = scores_all.merge(review_count, on=&#39;id&#39;).sort_values(by=[&#39;score_sentiment&#39;], ascending=True) . scores_all[:10] . id score score_sentiment text_count . 14 title182 | 1.500 | 2.500 | 2 | . 58 title697 | 4.667 | 2.833 | 6 | . 20 title207 | 2.000 | 3.000 | 2 | . 88 title83 | 1.000 | 3.000 | 2 | . 29 title31 | 3.750 | 3.000 | 4 | . 45 title582 | 3.000 | 3.000 | 2 | . 9 title167 | 3.176 | 3.235 | 17 | . 37 title507 | 3.056 | 3.284 | 162 | . 5 title152 | 4.167 | 3.333 | 6 | . 63 title75 | 4.273 | 3.333 | 33 | . Discover the reviews count . df_final.head() . id score text is_about_localization language language_full en_trans sentiment_vader compound score_sentiment is_pos . 0 title777 | 1 | æ²¡æœ‰åè¯­ç‰ˆæœ¬ | True | zh | Chinese | No Chinese version | {&#39;neg&#39;: 0.524, &#39;neu&#39;: 0.476, &#39;pos&#39;: 0.0, &#39;comp... | -0.2960 | 2 | 0 | . 1 title777 | 5 | æ²¡æœ‰ä¸­æ–‡å­—.å¾ˆéš¾ç©-å¾ˆå¥½ç©çš„æ¸¸æˆ.åªå¯æƒœè¯­è¨€ä¸æ˜ | True | zh | Chinese | No Chinese characters. Hard to play-very fun g... | {&#39;neg&#39;: 0.289, &#39;neu&#39;: 0.547, &#39;pos&#39;: 0.164, &#39;co... | -0.1280 | 3 | 0 | . 2 title777 | 5 | Excellent en franÃ§ais et connexion Facebook sa... | True | fr | French | Excellent in French and Facebook connection wi... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.654, &#39;pos&#39;: 0.346, &#39;comp... | 0.5719 | 4 | 1 | . 3 title777 | 5 | ì¬ë°Œì–´ìš”. ì˜ì–´ê¹Œë§‰ëˆˆì´ë¼ í•œêµ­ì–´ íŒ¨ì¹˜ ëìŒ ì¢‹ê² ì–´ìš”. | True | ko | Korean | fun. Itâ€™s English black eyes, so I hope that t... | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.63, &#39;pos&#39;: 0.37, &#39;compou... | 0.7930 | 5 | 1 | . 4 title777 | 5 | ì—°ë£Œ.. ë„ˆë¬´ì•ˆì£¼ê³ ã…œ í•œêµ­ì–´ë¡œì¢€ ë²ˆì—­ì¢€í•´ì£¼ì‹œì§€. .ì •ë³´ë„ì—†ê³ ã…œã…œ ì¼ìˆê¸´ ì¼ìˆìŒ ã…‹ã…‹ | True | ko | Korean | Don&#39;t give me fuel.. Please translate it into ... | {&#39;neg&#39;: 0.056, &#39;neu&#39;: 0.668, &#39;pos&#39;: 0.276, &#39;co... | 0.7391 | 5 | 1 | . . neg_review_count = df_final[df_final[&#39;is_pos&#39;]==0].groupby(&#39;id&#39;)[&#39;text&#39;].count().reset_index().sort_values(by=&#39;text&#39;, ascending=False) neg_review_count.head(10) . id text . 24 title249 | 2134 | . 70 title658 | 723 | . 109 title885 | 548 | . 75 title700 | 527 | . 40 title407 | 434 | . 105 title832 | 267 | . 73 title675 | 264 | . 81 title765 | 187 | . 66 title631 | 169 | . 69 title655 | 137 | . There are 10 games with most count of negative location reviews. Let&#39;s see what languages asked more. . Usually the primary language for a new game is an English, so I assume that people that wrote reviews in English are not asked about the translation. Of course it&#39;s not true, but let&#39;s do it to have a fast overview. . df_to_dashboard_negative_top = df_final[(df_final[&#39;id&#39;].isin(neg_review_count[&#39;id&#39;].tolist()[:10])) &amp; (df_final[&#39;is_pos&#39;]==0) &amp; (df_final[&#39;language_full&#39;]!=&#39;English&#39;)][[&#39;id&#39;, &#39;language_full&#39;, &#39;score_sentiment&#39;, &#39;is_pos&#39;]] . for i in neg_review_count[&#39;id&#39;].tolist()[:10]: df = df_final[(df_final[&#39;id&#39;]==i) &amp; (df_final[&#39;is_pos&#39;]==0) &amp; (df_final[&#39;language_full&#39;]!=&#39;English&#39;)].groupby(&#39;language_full&#39;)[&#39;text&#39;].count().reset_index().sort_values(by=&#39;text&#39;, ascending=False) print(&quot;game:&quot;, i) print(df.head(10)) print(&#39;&#39;) top_lang = df[&#39;language_full&#39;][:4] df_to_dashboard_negative_top[&#39;language_full&#39;] = np.where(~(df_to_dashboard_negative_top[&#39;language_full&#39;].isin(top_lang)) &amp; (df_to_dashboard_negative_top[&#39;id&#39;] == i), &#39;Other&#39;, df_to_dashboard_negative_top[&#39;language_full&#39;]) . game: title249 language_full text 0 Chinese 299 5 German 102 12 Russian 74 6 Indonesian 36 16 Turkish 31 4 French 22 9 Korean 17 13 Spanish 16 10 Polish 15 11 Portuguese 12 game: title658 language_full text 11 Russian 232 3 French 186 14 Turkish 112 6 Italian 63 4 German 42 8 Korean 23 12 Spanish 11 5 Indonesian 9 9 Polish 8 1 Chinese 7 game: title885 language_full text 15 Turkish 163 14 Thai 81 5 Indonesian 64 8 Korean 50 1 Chinese 40 0 Arabic 28 10 Polish 27 11 Portuguese 15 7 Japanese 14 12 Russian 14 game: title700 language_full text 20 Turkish 134 12 Korean 61 11 Japanese 58 0 Arabic 36 9 Indonesian 29 16 Russian 25 19 Thai 24 5 French 23 6 German 22 1 Chinese 18 game: title407 language_full text 0 Chinese 111 10 Russian 43 7 Korean 22 13 Turkish 15 4 French 9 6 Japanese 9 9 Portuguese 7 11 Spanish 6 8 Polish 5 1 Czech 1 game: title832 language_full text 8 Polish 46 1 Chinese 43 4 Indonesian 30 13 Turkish 30 7 Korean 21 5 Italian 16 12 Thai 15 0 Arabic 12 10 Russian 11 9 Portuguese 10 game: title675 language_full text 10 Russian 79 3 German 58 5 Italian 36 8 Polish 10 9 Portuguese 8 11 Spanish 8 2 French 5 6 Japanese 3 0 Chinese 2 7 Korean 2 game: title765 language_full text 15 Turkish 35 10 Polish 23 1 Chinese 19 9 Korean 18 5 German 12 6 Indonesian 12 12 Russian 11 16 Vietnamese 7 11 Portuguese 6 13 Spanish 5 game: title631 language_full text 1 Chinese 40 8 Korean 23 14 Turkish 22 5 Indonesian 11 9 Polish 10 13 Thai 10 10 Portuguese 7 4 German 5 7 Japanese 4 11 Russian 3 game: title655 language_full text 15 Turkish 66 0 Arabic 8 10 Polish 8 6 Indonesian 7 12 Russian 7 7 Italian 6 11 Portuguese 5 1 Chinese 4 9 Korean 4 13 Spanish 4 . . Top languages among localization reviews . 10 games with most count of reviews about localization . popular_games_without_loc = scores_all.sort_values(by=[&#39;text_count&#39;], ascending=False)[&#39;id&#39;][:10].tolist() . popular_games_without_loc . [&#39;title885&#39;, &#39;title658&#39;, &#39;title700&#39;, &#39;title832&#39;, &#39;title655&#39;, &#39;title675&#39;, &#39;title765&#39;, &#39;title631&#39;, &#39;title600&#39;, &#39;title782&#39;] . df_final[df_final[&#39;id&#39;].isin(popular_games_without_loc)].groupby(&#39;language_full&#39;)[&#39;language_full&#39;].count().sort_values(ascending=False) . language_full Turkish 3175 Portuguese 1649 English 1547 Thai 1480 Russian 1454 Indonesian 1204 French 898 Korean 809 Italian 792 Arabic 673 German 626 Chinese 624 Polish 550 Japanese 377 Spanish 281 Dutch 115 Vietnamese 69 Czech 53 Greek 41 Norwegian 18 Hebrew 12 Swedish 11 Danish 10 Finnish 9 Hindi 2 Name: language_full, dtype: int64 . df_final.groupby(&#39;language_full&#39;)[&#39;language_full&#39;].count().sort_values(ascending=False) . language_full English 8000 Turkish 4510 Portuguese 3755 Chinese 3260 Russian 2537 Thai 1729 Korean 1596 Indonesian 1576 French 1491 German 1390 Italian 1367 Spanish 875 Japanese 798 Arabic 794 Polish 767 Dutch 314 Czech 118 Vietnamese 109 Norwegian 49 Greek 46 Swedish 34 Finnish 33 Danish 32 Hebrew 21 Hindi 3 Name: language_full, dtype: int64 . Conclusion . We do not really know the reasons for negative reviews. VADER rely on words&#39; emotional, so the review is negative because the reviewer used negative emotion words. . We do know the sentiment of all reviews. So we can assume that a negative review tells about a problem with a language the review was written on. . The languages, the most localization reviews are written, are English, Turkish, Portuguese, Thai, Russian, Chinese, Korean, French, German, Italian and Indonesian. . Data Studio Dashboard .",
            "url": "https://pluzharovakp.github.io/ds_projects/2021/05/14/AllCorrect-Project.html",
            "relUrl": "/2021/05/14/AllCorrect-Project.html",
            "date": " â€¢ May 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Interconnect DS Project",
            "content": "Project Description . The telecom operator Interconnect would like to be able to forecast their churn of clients. If it&#39;s discovered that a user is planning to leave, they will be offered promotional codes and special plan options. Interconnect&#39;s marketing team has collected some of their clientele&#39;s personal data, including information about their plans and contracts. . Plan . Data Preprocessing Look at the data | Change data types | Join the tables | Make decition about NaN values | . | EDA Figure out what data is more relevant for prediction task and what is the target value | Try to find some pattern or new info that can be used | . | Models training Use cross-validation for better traning | Hyperparameter tuning | Choose the best model rest on the choosen metric (AUC-ROC) | . | Test the model | Conclusion | !pip3 install catboost . Collecting catboost Downloading https://files.pythonhosted.org/packages/96/3b/bb419654adcf7efff42ed8a3f84e50c8f236424b7ed1cc8ccd290852e003/catboost-0.24.4-cp37-none-manylinux1_x86_64.whl (65.7MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.7MB 71kB/s Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-0.24.4 . . import pandas as pd import numpy as np from lightgbm import LGBMRegressor, cv, Dataset from catboost import Pool, CatBoostRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import roc_auc_score from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder, OneHotEncoder from sklearn.model_selection import GridSearchCV from sklearn.metrics import make_scorer from sklearn.dummy import DummyRegressor from sklearn.ensemble import GradientBoostingRegressor . Data Preprocessing . contract = pd.read_csv(path_to_contract) internet = pd.read_csv(path_to_internet) personal = pd.read_csv(path_to_personal) phone = pd.read_csv(path_to_phone) . contract.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7043 entries, 0 to 7042 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 customerID 7043 non-null object 1 BeginDate 7043 non-null object 2 EndDate 7043 non-null object 3 Type 7043 non-null object 4 PaperlessBilling 7043 non-null object 5 PaymentMethod 7043 non-null object 6 MonthlyCharges 7043 non-null float64 7 TotalCharges 7043 non-null object dtypes: float64(1), object(7) memory usage: 440.3+ KB . internet.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5517 entries, 0 to 5516 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 customerID 5517 non-null object 1 InternetService 5517 non-null object 2 OnlineSecurity 5517 non-null object 3 OnlineBackup 5517 non-null object 4 DeviceProtection 5517 non-null object 5 TechSupport 5517 non-null object 6 StreamingTV 5517 non-null object 7 StreamingMovies 5517 non-null object dtypes: object(8) memory usage: 344.9+ KB . personal.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7043 entries, 0 to 7042 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 customerID 7043 non-null object 1 gender 7043 non-null object 2 SeniorCitizen 7043 non-null int64 3 Partner 7043 non-null object 4 Dependents 7043 non-null object dtypes: int64(1), object(4) memory usage: 275.2+ KB . phone.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6361 entries, 0 to 6360 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 customerID 6361 non-null object 1 MultipleLines 6361 non-null object dtypes: object(2) memory usage: 99.5+ KB . contract.head() . customerID BeginDate EndDate Type PaperlessBilling PaymentMethod MonthlyCharges TotalCharges . 0 7590-VHVEG | 2020-01-01 | No | Month-to-month | Yes | Electronic check | 29.85 | 29.85 | . 1 5575-GNVDE | 2017-04-01 | No | One year | No | Mailed check | 56.95 | 1889.5 | . 2 3668-QPYBK | 2019-10-01 | 2019-12-01 00:00:00 | Month-to-month | Yes | Mailed check | 53.85 | 108.15 | . 3 7795-CFOCW | 2016-05-01 | No | One year | No | Bank transfer (automatic) | 42.30 | 1840.75 | . 4 9237-HQITU | 2019-09-01 | 2019-11-01 00:00:00 | Month-to-month | Yes | Electronic check | 70.70 | 151.65 | . . internet.head() . customerID InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies . 0 7590-VHVEG | DSL | No | Yes | No | No | No | No | . 1 5575-GNVDE | DSL | Yes | No | Yes | No | No | No | . 2 3668-QPYBK | DSL | Yes | Yes | No | No | No | No | . 3 7795-CFOCW | DSL | Yes | No | Yes | Yes | No | No | . 4 9237-HQITU | Fiber optic | No | No | No | No | No | No | . . personal.head() . customerID gender SeniorCitizen Partner Dependents . 0 7590-VHVEG | Female | 0 | Yes | No | . 1 5575-GNVDE | Male | 0 | No | No | . 2 3668-QPYBK | Male | 0 | No | No | . 3 7795-CFOCW | Male | 0 | No | No | . 4 9237-HQITU | Female | 0 | No | No | . . phone.head() . customerID MultipleLines . 0 5575-GNVDE | No | . 1 3668-QPYBK | No | . 2 9237-HQITU | No | . 3 9305-CDSKC | Yes | . 4 1452-KIOVK | Yes | . . internet[&#39;FiberOptic&#39;] = (internet[&#39;InternetService&#39;] == &#39;Fiber optic&#39;).astype(int) internet = internet.drop(columns=[&#39;InternetService&#39;]) . df = contract.merge(phone, on=&#39;customerID&#39;, how=&#39;left&#39;) df = df.merge(internet, on=&#39;customerID&#39;, how=&#39;left&#39;) df = df.merge(personal, on=&#39;customerID&#39;, how=&#39;left&#39;) . df.head() . customerID BeginDate EndDate Type PaperlessBilling PaymentMethod MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic gender SeniorCitizen Partner Dependents . 0 7590-VHVEG | 2020-01-01 | No | Month-to-month | Yes | Electronic check | 29.85 | 29.85 | NaN | No | Yes | No | No | No | No | 0.0 | Female | 0 | Yes | No | . 1 5575-GNVDE | 2017-04-01 | No | One year | No | Mailed check | 56.95 | 1889.5 | No | Yes | No | Yes | No | No | No | 0.0 | Male | 0 | No | No | . 2 3668-QPYBK | 2019-10-01 | 2019-12-01 00:00:00 | Month-to-month | Yes | Mailed check | 53.85 | 108.15 | No | Yes | Yes | No | No | No | No | 0.0 | Male | 0 | No | No | . 3 7795-CFOCW | 2016-05-01 | No | One year | No | Bank transfer (automatic) | 42.30 | 1840.75 | NaN | Yes | No | Yes | Yes | No | No | 0.0 | Male | 0 | No | No | . 4 9237-HQITU | 2019-09-01 | 2019-11-01 00:00:00 | Month-to-month | Yes | Electronic check | 70.70 | 151.65 | No | No | No | No | No | No | No | 1.0 | Female | 0 | No | No | . df[&#39;TotalCharges&#39;] = pd.to_numeric(df[&#39;TotalCharges&#39;],errors=&#39;coerce&#39;) df[&#39;TotalCharges&#39;] = df[&#39;TotalCharges&#39;].round(2) . columns = [&#39;MultipleLines&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;FiberOptic&#39;, &#39;SeniorCitizen&#39;, &#39;PaperlessBilling&#39;, &#39;Partner&#39;, &#39;Dependents&#39;, &#39;TotalCharges&#39;,&#39;MonthlyCharges&#39;] for i in columns: df[i] = df[i].replace(&#39;No&#39;, 0) df[i] = df[i].replace(&#39;Yes&#39;, 1) df[i] = df[i].fillna(0) df[i] = df[i].astype(int) . df[&#39;BeginDate&#39;] = pd.to_datetime(df[&#39;BeginDate&#39;], format=&#39;%Y-%m-%d&#39;) . df[&#39;Woman&#39;] = (df[&#39;gender&#39;] == &#39;Female&#39;).astype(int) df = df.drop(columns=[&#39;gender&#39;]) . Need to explore ex-users and find something common between them. . For client who didn&#39;t exist in phone or internet tables NaN values should be fiiled with False value or 0 in the case. Probably they didn&#39;t use those services. . EDA . df[&#39;EndDate&#39;].value_counts() . No 5174 2019-11-01 00:00:00 485 2019-12-01 00:00:00 466 2020-01-01 00:00:00 460 2019-10-01 00:00:00 458 Name: EndDate, dtype: int64 . current_users = df.loc[df[&#39;EndDate&#39;] == &#39;No&#39;] . ex_users = df.loc[df[&#39;EndDate&#39;] != &#39;No&#39;] . ex_users[&#39;BeginDate&#39;] = ex_users[&#39;BeginDate&#39;].dt.year current_users[&#39;BeginDate&#39;] = current_users[&#39;BeginDate&#39;].dt.year . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . . ex_users[&#39;BeginDate&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff9c9338dd0&gt; . There are a lot of new users from 2019 year who probably decided to try, but the service didn&#39;t sute them for some reason. . arg_difference = {&#39;Ex-Users&#39;: ex_users.mean(), &#39;Current-Users&#39;: current_users.mean()} arg_difference_df = pd.DataFrame(data=arg_difference) . arg_difference_df . Ex-Users Current-Users . BeginDate 2017.834671 | 2016.533243 | . PaperlessBilling 0.749064 | 0.535562 | . MonthlyCharges 73.979668 | 60.797835 | . TotalCharges 1531.323167 | 2549.443564 | . MultipleLines 0.454789 | 0.409934 | . OnlineSecurity 0.157838 | 0.333204 | . OnlineBackup 0.279829 | 0.368380 | . DeviceProtection 0.291600 | 0.362775 | . TechSupport 0.165864 | 0.335137 | . StreamingTV 0.435527 | 0.365868 | . StreamingMovies 0.437667 | 0.369927 | . FiberOptic 0.693954 | 0.347700 | . SeniorCitizen 0.254682 | 0.128721 | . Partner 0.357945 | 0.528218 | . Dependents 0.174425 | 0.344801 | . Woman 0.502408 | 0.492656 | . ex_users.groupby(&#39;BeginDate&#39;).mean() . PaperlessBilling MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic SeniorCitizen Partner Dependents Woman . BeginDate . 2013 0.750000 | 100.125000 | 7137.750000 | 1.000000 | 0.625000 | 0.625000 | 0.875000 | 0.750000 | 0.875000 | 0.875000 | 0.750000 | 0.250000 | 0.875000 | 0.250000 | 0.500000 | . 2014 0.775510 | 95.724490 | 6316.387755 | 0.826531 | 0.326531 | 0.734694 | 0.663265 | 0.387755 | 0.836735 | 0.846939 | 0.846939 | 0.255102 | 0.724490 | 0.295918 | 0.367347 | . 2015 0.756098 | 88.000000 | 4646.040650 | 0.747967 | 0.325203 | 0.560976 | 0.512195 | 0.243902 | 0.731707 | 0.699187 | 0.796748 | 0.308943 | 0.601626 | 0.203252 | 0.471545 | . 2016 0.813793 | 83.882759 | 3441.834483 | 0.600000 | 0.262069 | 0.475862 | 0.510345 | 0.317241 | 0.613793 | 0.627586 | 0.744828 | 0.365517 | 0.482759 | 0.172414 | 0.503448 | . 2017 0.804020 | 82.884422 | 2375.693467 | 0.643216 | 0.190955 | 0.381910 | 0.361809 | 0.185930 | 0.537688 | 0.562814 | 0.824121 | 0.331658 | 0.492462 | 0.236181 | 0.532663 | . 2018 0.790476 | 78.352381 | 1254.796825 | 0.517460 | 0.187302 | 0.269841 | 0.314286 | 0.190476 | 0.498413 | 0.501587 | 0.752381 | 0.279365 | 0.387302 | 0.184127 | 0.517460 | . 2019 0.711519 | 65.162080 | 252.173293 | 0.296636 | 0.084608 | 0.149847 | 0.168196 | 0.094801 | 0.287462 | 0.286442 | 0.612640 | 0.207951 | 0.231397 | 0.142712 | 0.508665 | . current_users.groupby(&#39;BeginDate&#39;).mean() . PaperlessBilling MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic SeniorCitizen Partner Dependents Woman . BeginDate . 2014 0.581059 | 74.090690 | 5121.858748 | 0.662921 | 0.542536 | 0.605939 | 0.600321 | 0.539326 | 0.561798 | 0.570626 | 0.418941 | 0.155698 | 0.763242 | 0.413323 | 0.483949 | . 2015 0.578875 | 67.499314 | 3785.851852 | 0.508916 | 0.393690 | 0.462277 | 0.452675 | 0.406036 | 0.462277 | 0.475995 | 0.407407 | 0.153635 | 0.639232 | 0.370370 | 0.511660 | . 2016 0.550162 | 62.250809 | 2746.218447 | 0.428803 | 0.330097 | 0.385113 | 0.399676 | 0.317152 | 0.401294 | 0.401294 | 0.364078 | 0.119741 | 0.548544 | 0.360841 | 0.525890 | . 2017 0.518576 | 60.730650 | 1913.739938 | 0.380805 | 0.297214 | 0.329721 | 0.349845 | 0.326625 | 0.349845 | 0.345201 | 0.362229 | 0.147059 | 0.487616 | 0.329721 | 0.470588 | . 2018 0.511888 | 54.158042 | 1074.416783 | 0.309091 | 0.236364 | 0.236364 | 0.236364 | 0.237762 | 0.261538 | 0.254545 | 0.318881 | 0.114685 | 0.446154 | 0.335664 | 0.492308 | . 2019 0.498975 | 48.607582 | 347.054303 | 0.175205 | 0.177254 | 0.178279 | 0.148566 | 0.170082 | 0.184426 | 0.191598 | 0.267418 | 0.099385 | 0.309426 | 0.272541 | 0.501025 | . 2020 0.397541 | 37.610656 | 35.766393 | 0.086066 | 0.094262 | 0.081967 | 0.049180 | 0.094262 | 0.061475 | 0.065574 | 0.131148 | 0.049180 | 0.168033 | 0.233607 | 0.422131 | . Conclution of EDA: . Our ex-clients usually had internet with Fiber Optic, used Streaming TV and Streaming Movies and as an obvious fact paid more. They also less used Tech Support, more of them used Multiple Lines of the phone and there are more SeniorCitizen among ex-users. . Partner and Gender information don&#39;t seem to be relevant, but Dependents can be as there are some difference by this parameter. . Most of the ex-users came to us in 2019 year. . Questions: . What do Partner and Dependents columns mean? Are they relevant? | What the most relevant columns would be the best to take as the features for machine leaning model? | What is the target value? | It is a classification task: whether or not the user will quit, isn&#39;t it? | What metric to use? | Our target column is EndDate. As far as it should has boolean type (with us or not), so the column values should be changed. . It&#39;s better to remain all features as all of them can be relevant to the customer decision to leave or to stay. . It is a classification task, but Logistic regression can be used as there are only two target options. . AUC-ROC metric should be used here to find the best model, because our data is unballanced and it needs to take in account the rate of false-positive and false-negative values. . Model Training . Creating Features . df[&#39;IsClient&#39;] = (df[&#39;EndDate&#39;] == &#39;No&#39;).astype(int) . df[&#39;EndDate&#39;] = df[&#39;EndDate&#39;].where(df[&#39;EndDate&#39;] != &#39;No&#39;, &quot;2020-02-01&quot;) df[&#39;EndDate&#39;] = pd.to_datetime(df[&#39;EndDate&#39;], format=&#39;%Y-%m-%d&#39;) . df[&quot;AllTime&quot;] = (df[&#39;EndDate&#39;] - df[&#39;BeginDate&#39;]) df[&quot;AllTime&quot;] = (df[&quot;AllTime&quot;] / np.timedelta64(1, &quot;M&quot;)) df[&quot;AllTime&quot;] = df[&quot;AllTime&quot;].astype(int) . df.head() . customerID BeginDate EndDate Type PaperlessBilling PaymentMethod MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic SeniorCitizen Partner Dependents Woman IsClient AllTime . 0 7590-VHVEG | 2020-01-01 | 2020-02-01 | Month-to-month | 1 | Electronic check | 29 | 29 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | . 1 5575-GNVDE | 2017-04-01 | 2020-02-01 | One year | 0 | Mailed check | 56 | 1889 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 34 | . 2 3668-QPYBK | 2019-10-01 | 2019-12-01 | Month-to-month | 1 | Mailed check | 53 | 108 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 3 7795-CFOCW | 2016-05-01 | 2020-02-01 | One year | 0 | Bank transfer (automatic) | 42 | 1840 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 45 | . 4 9237-HQITU | 2019-09-01 | 2019-11-01 | Month-to-month | 1 | Electronic check | 70 | 151 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 2 | . df.columns . Index([&#39;customerID&#39;, &#39;BeginDate&#39;, &#39;EndDate&#39;, &#39;Type&#39;, &#39;PaperlessBilling&#39;, &#39;PaymentMethod&#39;, &#39;MonthlyCharges&#39;, &#39;TotalCharges&#39;, &#39;MultipleLines&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;FiberOptic&#39;, &#39;SeniorCitizen&#39;, &#39;Partner&#39;, &#39;Dependents&#39;, &#39;Woman&#39;, &#39;IsClient&#39;, &#39;AllTime&#39;], dtype=&#39;object&#39;) . X = df.drop([&#39;customerID&#39;, &#39;BeginDate&#39;, &#39;EndDate&#39;,&#39;Partner&#39;, &#39;Woman&#39;, &#39;IsClient&#39;], axis=1) y = df[&#39;IsClient&#39;] . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 7043 entries, 0 to 7042 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 Type 7043 non-null object 1 PaperlessBilling 7043 non-null int64 2 PaymentMethod 7043 non-null object 3 MonthlyCharges 7043 non-null int64 4 TotalCharges 7043 non-null int64 5 MultipleLines 7043 non-null int64 6 OnlineSecurity 7043 non-null int64 7 OnlineBackup 7043 non-null int64 8 DeviceProtection 7043 non-null int64 9 TechSupport 7043 non-null int64 10 StreamingTV 7043 non-null int64 11 StreamingMovies 7043 non-null int64 12 FiberOptic 7043 non-null int64 13 SeniorCitizen 7043 non-null int64 14 Dependents 7043 non-null int64 15 AllTime 7043 non-null int64 dtypes: int64(14), object(2) memory usage: 935.4+ KB . encoder = OrdinalEncoder() X = pd.DataFrame(encoder.fit_transform(X), columns = X.columns) . X.head() . Type PaperlessBilling PaymentMethod MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic SeniorCitizen Dependents AllTime . 0 0.0 | 1.0 | 2.0 | 10.0 | 10.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 1 1.0 | 0.0 | 3.0 | 36.0 | 1539.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 34.0 | . 2 0.0 | 1.0 | 3.0 | 33.0 | 89.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | . 3 1.0 | 0.0 | 0.0 | 22.0 | 1503.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 45.0 | . 4 0.0 | 1.0 | 2.0 | 50.0 | 131.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.0 | . for i in X.columns: X[i] = X[i].astype(&#39;int&#39;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345) . X.head() . Type PaperlessBilling PaymentMethod MonthlyCharges TotalCharges MultipleLines OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies FiberOptic SeniorCitizen Dependents AllTime . 0 0 | 1 | 2 | 10 | 10 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 1 1 | 0 | 3 | 36 | 1539 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 34 | . 2 0 | 1 | 3 | 33 | 89 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | . 3 1 | 0 | 0 | 22 | 1503 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 45 | . 4 0 | 1 | 2 | 50 | 131 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | . Dummy model . scorer_for_cv = make_scorer(roc_auc_score) . dummy_regr = DummyRegressor(strategy=&quot;constant&quot;, constant=0) dummy_regr.fit(X_train, y_train) dummy_predict = dummy_regr.predict(X_test) . dummy_auc = roc_auc_score(y_test, dummy_predict) print(&quot;AUC-ROC =&quot;, dummy_auc) . AUC-ROC = 0.5 . Linear regression . model_lr = LinearRegression() param_grid = [{&#39;normalize&#39;: [True, False]}] model_lr_cv = GridSearchCV(estimator=model_lr, scoring=scorer_for_cv, param_grid=param_grid, cv=3) model_lr_cv = model_lr_cv.fit(X_train, y_train) . model_lr_cv.best_score_ . 0.9134962791064755 . Random forrest . model_rfr = RandomForestRegressor() param_grid = [ {&#39;n_estimators&#39;:[20], &#39;max_depth&#39;:[12], &#39;random_state&#39;:[12345]}, {&#39;n_estimators&#39;:[50], &#39;max_depth&#39;:[5],&#39;random_state&#39;:[12345]}, {&#39;n_estimators&#39;:[50],&#39;max_depth&#39;:[50], &#39;random_state&#39;:[12345]}] model_rfr_cv = GridSearchCV(estimator=model_rfr, param_grid=param_grid, scoring=scorer_for_cv, cv=5) model_rfr_cv = model_rfr_cv.fit(X_train, y_train) . model_rfr_cv.best_score_ . 0.9228055321689848 . Gradient boosting . train_pool = Pool(X_train, y_train, cat_features=[&#39;PaperlessBilling&#39;, &#39;TotalCharges&#39;, &#39;MultipleLines&#39;, &#39;MonthlyCharges&#39;,&#39;AllTime&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;FiberOptic&#39;, &#39;SeniorCitizen&#39;, &#39;Dependents&#39;]) model_CatBoostRegressor = CatBoostRegressor(eval_metric=&#39;AUC&#39;) . param_grid = [ {&#39;iterations&#39;: [5, 10, 100], &#39;depth&#39;:[5, 10, 15], &#39;learning_rate&#39;:[1]}] model_cb_cv = model_CatBoostRegressor.grid_search(param_grid, X=train_pool, cv=3) . bestTest = 0.8371061337 bestIteration = 4 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 0: loss: 0.8371061 best: 0.8371061 (0) total: 76.8ms remaining: 615ms bestTest = 0.8401220378 bestIteration = 6 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 1: loss: 0.8401220 best: 0.8401220 (1) total: 102ms remaining: 358ms bestTest = 0.8451820289 bestIteration = 14 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 2: loss: 0.8451820 best: 0.8451820 (2) total: 316ms remaining: 633ms bestTest = 0.826336716 bestIteration = 3 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 3: loss: 0.8263367 best: 0.8451820 (2) total: 374ms remaining: 468ms bestTest = 0.8264395044 bestIteration = 8 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 4: loss: 0.8264395 best: 0.8451820 (2) total: 475ms remaining: 380ms bestTest = 0.8264395044 bestIteration = 8 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 5: loss: 0.8264395 best: 0.8451820 (2) total: 1.7s remaining: 852ms bestTest = 0.8060803992 bestIteration = 1 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 6: loss: 0.8060804 best: 0.8451820 (2) total: 3.31s remaining: 945ms bestTest = 0.8060803992 bestIteration = 1 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 7: loss: 0.8060804 best: 0.8451820 (2) total: 5.42s remaining: 678ms bestTest = 0.8060803992 bestIteration = 1 Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters. 8: loss: 0.8060804 best: 0.8451820 (2) total: 34.2s remaining: 0us Estimating final quality... . . print(&quot;The best parameters is&quot;, model_cb_cv[&#39;params&#39;]) . The best parameters is {&#39;depth&#39;: 5, &#39;learning_rate&#39;: 1, &#39;iterations&#39;: 100} . model_cb_cv = CatBoostRegressor(iterations=100, depth=5, learning_rate=1, eval_metric=&#39;AUC&#39;) model_cb_cv.fit(train_pool) . 0: total: 3.66ms remaining: 363ms 1: total: 6ms remaining: 294ms 2: total: 8.94ms remaining: 289ms 3: total: 11.6ms remaining: 279ms 4: total: 13.9ms remaining: 264ms 5: total: 16.3ms remaining: 255ms 6: total: 18.7ms remaining: 248ms 7: total: 21ms remaining: 242ms 8: total: 23.4ms remaining: 237ms 9: total: 25.7ms remaining: 231ms 10: total: 28.1ms remaining: 227ms 11: total: 30.4ms remaining: 223ms 12: total: 32.7ms remaining: 219ms 13: total: 34.9ms remaining: 215ms 14: total: 37.2ms remaining: 211ms 15: total: 39.4ms remaining: 207ms 16: total: 41.6ms remaining: 203ms 17: total: 44.2ms remaining: 201ms 18: total: 46.5ms remaining: 198ms 19: total: 48.7ms remaining: 195ms 20: total: 51ms remaining: 192ms 21: total: 53.3ms remaining: 189ms 22: total: 55.6ms remaining: 186ms 23: total: 57.9ms remaining: 183ms 24: total: 60.6ms remaining: 182ms 25: total: 63ms remaining: 179ms 26: total: 65.5ms remaining: 177ms 27: total: 67.9ms remaining: 175ms 28: total: 70.3ms remaining: 172ms 29: total: 72.6ms remaining: 169ms 30: total: 75.1ms remaining: 167ms 31: total: 77.3ms remaining: 164ms 32: total: 79.8ms remaining: 162ms 33: total: 82.1ms remaining: 159ms 34: total: 84.5ms remaining: 157ms 35: total: 86.9ms remaining: 155ms 36: total: 89.3ms remaining: 152ms 37: total: 91.7ms remaining: 150ms 38: total: 94ms remaining: 147ms 39: total: 96.4ms remaining: 145ms 40: total: 98.7ms remaining: 142ms 41: total: 101ms remaining: 140ms 42: total: 103ms remaining: 137ms 43: total: 106ms remaining: 135ms 44: total: 108ms remaining: 132ms 45: total: 111ms remaining: 130ms 46: total: 113ms remaining: 127ms 47: total: 115ms remaining: 125ms 48: total: 118ms remaining: 123ms 49: total: 120ms remaining: 120ms 50: total: 123ms remaining: 118ms 51: total: 125ms remaining: 116ms 52: total: 128ms remaining: 113ms 53: total: 130ms remaining: 111ms 54: total: 132ms remaining: 108ms 55: total: 135ms remaining: 106ms 56: total: 137ms remaining: 103ms 57: total: 140ms remaining: 101ms 58: total: 142ms remaining: 98.7ms 59: total: 144ms remaining: 96.2ms 60: total: 147ms remaining: 93.8ms 61: total: 149ms remaining: 91.3ms 62: total: 151ms remaining: 89ms 63: total: 154ms remaining: 86.5ms 64: total: 156ms remaining: 84.1ms 65: total: 159ms remaining: 81.7ms 66: total: 161ms remaining: 79.2ms 67: total: 167ms remaining: 78.4ms 68: total: 169ms remaining: 76ms 69: total: 172ms remaining: 73.5ms 70: total: 176ms remaining: 72ms 71: total: 182ms remaining: 70.9ms 72: total: 186ms remaining: 68.8ms 73: total: 190ms remaining: 66.6ms 74: total: 195ms remaining: 65ms 75: total: 197ms remaining: 62.3ms 76: total: 200ms remaining: 59.6ms 77: total: 202ms remaining: 57ms 78: total: 204ms remaining: 54.3ms 79: total: 207ms remaining: 51.7ms 80: total: 209ms remaining: 49ms 81: total: 211ms remaining: 46.4ms 82: total: 214ms remaining: 43.8ms 83: total: 216ms remaining: 41.1ms 84: total: 218ms remaining: 38.5ms 85: total: 221ms remaining: 35.9ms 86: total: 223ms remaining: 33.3ms 87: total: 225ms remaining: 30.7ms 88: total: 228ms remaining: 28.1ms 89: total: 230ms remaining: 25.5ms 90: total: 232ms remaining: 23ms 91: total: 235ms remaining: 20.4ms 92: total: 237ms remaining: 17.8ms 93: total: 239ms remaining: 15.3ms 94: total: 242ms remaining: 12.7ms 95: total: 244ms remaining: 10.2ms 96: total: 246ms remaining: 7.62ms 97: total: 249ms remaining: 5.08ms 98: total: 251ms remaining: 2.54ms 99: total: 254ms remaining: 0us . &lt;catboost.core.CatBoostRegressor at 0x7ff9bcacaed0&gt; . . Test the model . Linear regression . y_pred = model_lr_cv.predict(X_test) lr_auc = roc_auc_score(y_test, y_pred) print(&quot;AUC-ROC =&quot;, lr_auc) . AUC-ROC = 0.8293950540785984 . Random forrest . y_pred = model_rfr_cv.predict(X_test) rfr_auc = roc_auc_score(y_test, y_pred) print(&quot;AUC-ROC =&quot;, rfr_auc) . AUC-ROC = 0.8531534101154354 . CatBoostClassifier . test_pool = Pool(X_test, cat_features=[&#39;PaperlessBilling&#39;, &#39;MonthlyCharges&#39;, &#39;TotalCharges&#39;, &#39;MultipleLines&#39;, &#39;OnlineSecurity&#39;, &#39;OnlineBackup&#39;, &#39;DeviceProtection&#39;, &#39;TechSupport&#39;, &#39;StreamingTV&#39;, &#39;StreamingMovies&#39;, &#39;FiberOptic&#39;, &#39;SeniorCitizen&#39;, &#39;Dependents&#39;, &#39;AllTime&#39;]) y_pred = model_cb_cv.predict(test_pool) print(y_pred) cbc_auc = roc_auc_score(y_test, y_pred) print(&quot;AUC-ROC =&quot;, cbc_auc) . [0.88648241 0.46205313 0.10287211 ... 0.79784994 1.16184092 1.02592976] AUC-ROC = 0.8272001088456784 . Solution Report . Question: What steps of the plan were performed and what steps were skipped (explain why)? | Answer: It was no need to skip any steps in the plan, so all steps were performed. | . . Question: What difficulties did you encounter and how did you manage to solve them? | Answer: The difficult and unsucceded part was to increase the AUC-ROC metric more than 0.88. | . . Question: What were some of the key steps to solving the task? | Answer: The key steps were to prepare data for model training and to find the best hyperparameters. | . . Question: What is your final model and what quality score does it have? | Answer: The Random Forest model gave the best result - it&#39;s AUC-ROC metric has value 0,853. It is the best model to predict the churn of clients. | .",
            "url": "https://pluzharovakp.github.io/ds_projects/2021/03/10/_Practicum-Final-Project.ipynb.html",
            "relUrl": "/2021/03/10/_Practicum-Final-Project.ipynb.html",
            "date": " â€¢ Mar 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Gold mining DS Project",
            "content": "Task statement . Prepare a prototype of a machine learning model for Zyfra. The company develops efficiency solutions for heavy industry. . The model should predict the amount of gold recovered from gold ore. There are the data on extraction and purification. . The model will help to optimize the production and eliminate unprofitable parameters. . Plan . Data Preprocessing Look at the data | Check that recovery calculation | Analyze the test set | Make decition about NaN values | . | EDA Dependence between concentrations of metals (Au, Ag, Pb) and purification stage | Compare the feed particle size distributions in the train and in the test sets | Consider the total concentrations of all substances at different stages | . | Train the model | Test the model | Conclusion | import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from numpy.random import RandomState from scipy import stats as st import matplotlib.pyplot as plt from sklearn import preprocessing import numpy as np from sklearn.dummy import DummyRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import make_scorer . Data Preprocessing . Look at the data . train = pd.read_csv(&quot;/datasets/gold_recovery_train.csv&quot;) test = pd.read_csv(&quot;/datasets/gold_recovery_test.csv&quot;) full = pd.read_csv(&quot;/datasets/gold_recovery_full.csv&quot;) . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 87 columns): date 16860 non-null object final.output.concentrate_ag 16788 non-null float64 final.output.concentrate_pb 16788 non-null float64 final.output.concentrate_sol 16490 non-null float64 final.output.concentrate_au 16789 non-null float64 final.output.recovery 15339 non-null float64 final.output.tail_ag 16794 non-null float64 final.output.tail_pb 16677 non-null float64 final.output.tail_sol 16715 non-null float64 final.output.tail_au 16794 non-null float64 primary_cleaner.input.sulfate 15553 non-null float64 primary_cleaner.input.depressant 15598 non-null float64 primary_cleaner.input.feed_size 16860 non-null float64 primary_cleaner.input.xanthate 15875 non-null float64 primary_cleaner.output.concentrate_ag 16778 non-null float64 primary_cleaner.output.concentrate_pb 16502 non-null float64 primary_cleaner.output.concentrate_sol 16224 non-null float64 primary_cleaner.output.concentrate_au 16778 non-null float64 primary_cleaner.output.tail_ag 16777 non-null float64 primary_cleaner.output.tail_pb 16761 non-null float64 primary_cleaner.output.tail_sol 16579 non-null float64 primary_cleaner.output.tail_au 16777 non-null float64 primary_cleaner.state.floatbank8_a_air 16820 non-null float64 primary_cleaner.state.floatbank8_a_level 16827 non-null float64 primary_cleaner.state.floatbank8_b_air 16820 non-null float64 primary_cleaner.state.floatbank8_b_level 16833 non-null float64 primary_cleaner.state.floatbank8_c_air 16822 non-null float64 primary_cleaner.state.floatbank8_c_level 16833 non-null float64 primary_cleaner.state.floatbank8_d_air 16821 non-null float64 primary_cleaner.state.floatbank8_d_level 16833 non-null float64 rougher.calculation.sulfate_to_au_concentrate 16833 non-null float64 rougher.calculation.floatbank10_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.floatbank11_sulfate_to_au_feed 16833 non-null float64 rougher.calculation.au_pb_ratio 15618 non-null float64 rougher.input.feed_ag 16778 non-null float64 rougher.input.feed_pb 16632 non-null float64 rougher.input.feed_rate 16347 non-null float64 rougher.input.feed_size 16443 non-null float64 rougher.input.feed_sol 16568 non-null float64 rougher.input.feed_au 16777 non-null float64 rougher.input.floatbank10_sulfate 15816 non-null float64 rougher.input.floatbank10_xanthate 16514 non-null float64 rougher.input.floatbank11_sulfate 16237 non-null float64 rougher.input.floatbank11_xanthate 14956 non-null float64 rougher.output.concentrate_ag 16778 non-null float64 rougher.output.concentrate_pb 16778 non-null float64 rougher.output.concentrate_sol 16698 non-null float64 rougher.output.concentrate_au 16778 non-null float64 rougher.output.recovery 14287 non-null float64 rougher.output.tail_ag 14610 non-null float64 rougher.output.tail_pb 16778 non-null float64 rougher.output.tail_sol 14611 non-null float64 rougher.output.tail_au 14611 non-null float64 rougher.state.floatbank10_a_air 16807 non-null float64 rougher.state.floatbank10_a_level 16807 non-null float64 rougher.state.floatbank10_b_air 16807 non-null float64 rougher.state.floatbank10_b_level 16807 non-null float64 rougher.state.floatbank10_c_air 16807 non-null float64 rougher.state.floatbank10_c_level 16814 non-null float64 rougher.state.floatbank10_d_air 16802 non-null float64 rougher.state.floatbank10_d_level 16809 non-null float64 rougher.state.floatbank10_e_air 16257 non-null float64 rougher.state.floatbank10_e_level 16809 non-null float64 rougher.state.floatbank10_f_air 16802 non-null float64 rougher.state.floatbank10_f_level 16802 non-null float64 secondary_cleaner.output.tail_ag 16776 non-null float64 secondary_cleaner.output.tail_pb 16764 non-null float64 secondary_cleaner.output.tail_sol 14874 non-null float64 secondary_cleaner.output.tail_au 16778 non-null float64 secondary_cleaner.state.floatbank2_a_air 16497 non-null float64 secondary_cleaner.state.floatbank2_a_level 16751 non-null float64 secondary_cleaner.state.floatbank2_b_air 16705 non-null float64 secondary_cleaner.state.floatbank2_b_level 16748 non-null float64 secondary_cleaner.state.floatbank3_a_air 16763 non-null float64 secondary_cleaner.state.floatbank3_a_level 16747 non-null float64 secondary_cleaner.state.floatbank3_b_air 16752 non-null float64 secondary_cleaner.state.floatbank3_b_level 16750 non-null float64 secondary_cleaner.state.floatbank4_a_air 16731 non-null float64 secondary_cleaner.state.floatbank4_a_level 16747 non-null float64 secondary_cleaner.state.floatbank4_b_air 16768 non-null float64 secondary_cleaner.state.floatbank4_b_level 16767 non-null float64 secondary_cleaner.state.floatbank5_a_air 16775 non-null float64 secondary_cleaner.state.floatbank5_a_level 16775 non-null float64 secondary_cleaner.state.floatbank5_b_air 16775 non-null float64 secondary_cleaner.state.floatbank5_b_level 16776 non-null float64 secondary_cleaner.state.floatbank6_a_air 16757 non-null float64 secondary_cleaner.state.floatbank6_a_level 16775 non-null float64 dtypes: float64(86), object(1) memory usage: 11.2+ MB . . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5856 entries, 0 to 5855 Data columns (total 53 columns): date 5856 non-null object primary_cleaner.input.sulfate 5554 non-null float64 primary_cleaner.input.depressant 5572 non-null float64 primary_cleaner.input.feed_size 5856 non-null float64 primary_cleaner.input.xanthate 5690 non-null float64 primary_cleaner.state.floatbank8_a_air 5840 non-null float64 primary_cleaner.state.floatbank8_a_level 5840 non-null float64 primary_cleaner.state.floatbank8_b_air 5840 non-null float64 primary_cleaner.state.floatbank8_b_level 5840 non-null float64 primary_cleaner.state.floatbank8_c_air 5840 non-null float64 primary_cleaner.state.floatbank8_c_level 5840 non-null float64 primary_cleaner.state.floatbank8_d_air 5840 non-null float64 primary_cleaner.state.floatbank8_d_level 5840 non-null float64 rougher.input.feed_ag 5840 non-null float64 rougher.input.feed_pb 5840 non-null float64 rougher.input.feed_rate 5816 non-null float64 rougher.input.feed_size 5834 non-null float64 rougher.input.feed_sol 5789 non-null float64 rougher.input.feed_au 5840 non-null float64 rougher.input.floatbank10_sulfate 5599 non-null float64 rougher.input.floatbank10_xanthate 5733 non-null float64 rougher.input.floatbank11_sulfate 5801 non-null float64 rougher.input.floatbank11_xanthate 5503 non-null float64 rougher.state.floatbank10_a_air 5839 non-null float64 rougher.state.floatbank10_a_level 5840 non-null float64 rougher.state.floatbank10_b_air 5839 non-null float64 rougher.state.floatbank10_b_level 5840 non-null float64 rougher.state.floatbank10_c_air 5839 non-null float64 rougher.state.floatbank10_c_level 5840 non-null float64 rougher.state.floatbank10_d_air 5839 non-null float64 rougher.state.floatbank10_d_level 5840 non-null float64 rougher.state.floatbank10_e_air 5839 non-null float64 rougher.state.floatbank10_e_level 5840 non-null float64 rougher.state.floatbank10_f_air 5839 non-null float64 rougher.state.floatbank10_f_level 5840 non-null float64 secondary_cleaner.state.floatbank2_a_air 5836 non-null float64 secondary_cleaner.state.floatbank2_a_level 5840 non-null float64 secondary_cleaner.state.floatbank2_b_air 5833 non-null float64 secondary_cleaner.state.floatbank2_b_level 5840 non-null float64 secondary_cleaner.state.floatbank3_a_air 5822 non-null float64 secondary_cleaner.state.floatbank3_a_level 5840 non-null float64 secondary_cleaner.state.floatbank3_b_air 5840 non-null float64 secondary_cleaner.state.floatbank3_b_level 5840 non-null float64 secondary_cleaner.state.floatbank4_a_air 5840 non-null float64 secondary_cleaner.state.floatbank4_a_level 5840 non-null float64 secondary_cleaner.state.floatbank4_b_air 5840 non-null float64 secondary_cleaner.state.floatbank4_b_level 5840 non-null float64 secondary_cleaner.state.floatbank5_a_air 5840 non-null float64 secondary_cleaner.state.floatbank5_a_level 5840 non-null float64 secondary_cleaner.state.floatbank5_b_air 5840 non-null float64 secondary_cleaner.state.floatbank5_b_level 5840 non-null float64 secondary_cleaner.state.floatbank6_a_air 5840 non-null float64 secondary_cleaner.state.floatbank6_a_level 5840 non-null float64 dtypes: float64(52), object(1) memory usage: 2.4+ MB . . Check that recovery is calculated correctly . rougher_table = train[[&#39;rougher.input.feed_au&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.tail_au&#39;, &#39;rougher.output.recovery&#39;]] . rougher_table.columns = [&#39;feed&#39;, &#39;concentrate&#39;, &#39;tail&#39;, &#39;recovery&#39;] . rougher_table.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16860 entries, 0 to 16859 Data columns (total 4 columns): feed 16777 non-null float64 concentrate 16778 non-null float64 tail 14611 non-null float64 recovery 14287 non-null float64 dtypes: float64(4) memory usage: 527.0 KB . rougher_table[[&#39;feed&#39;, &#39;concentrate&#39;, &#39;tail&#39;, &#39;recovery&#39;]].isnull().mean() . feed 0.004923 concentrate 0.004864 tail 0.133393 recovery 0.152610 dtype: float64 . rougher_table = rougher_table.dropna().reset_index() . rougher_table.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 14287 entries, 0 to 14286 Data columns (total 5 columns): index 14287 non-null int64 feed 14287 non-null float64 concentrate 14287 non-null float64 tail 14287 non-null float64 recovery 14287 non-null float64 dtypes: float64(4), int64(1) memory usage: 558.2 KB . rougher_table.head() . index feed concentrate tail recovery . 0 | 0 | 6.486150 | 19.793808 | 1.170244 | 87.107763 | . 1 | 1 | 6.478583 | 20.050975 | 1.184827 | 86.843261 | . 2 | 2 | 6.362222 | 19.737170 | 1.162562 | 86.842308 | . 3 | 3 | 6.118189 | 19.320810 | 1.079755 | 87.226430 | . 4 | 4 | 5.663707 | 19.216101 | 1.012642 | 86.688794 | . def recovery_cal(table): r = [] for i in range(len(table)): c = table.loc[i, &#39;concentrate&#39;] f = table.loc[i, &#39;feed&#39;] t = table.loc[i, &#39;tail&#39;] if (f*(c-t)) == 0: r.append(0) else: result = (c*(f-t)) / (f*(c-t)) * 100 r.append(result) return r . rougher_table[&#39;r_pred&#39;] = recovery_cal(rougher_table) rougher_table[&#39;r_pred&#39;] = rougher_table[&#39;r_pred&#39;].fillna(value=0) . mae = mean_squared_error(rougher_table[&#39;recovery&#39;], rougher_table[&#39;r_pred&#39;]) mae . 2.0435431534920925e-28 . To re-calculate recovery it&#39;s better to drop all NaN value from the table to avoid errors. . In general original recovery value is not so different from re-calculated ones. . Analyze the test set . not_in_test = [x for x in train.columns if x not in test.columns] not_in_test . [&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.concentrate_au&#39;, &#39;final.output.recovery&#39;, &#39;final.output.tail_ag&#39;, &#39;final.output.tail_pb&#39;, &#39;final.output.tail_sol&#39;, &#39;final.output.tail_au&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_sol&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;primary_cleaner.output.tail_ag&#39;, &#39;primary_cleaner.output.tail_pb&#39;, &#39;primary_cleaner.output.tail_sol&#39;, &#39;primary_cleaner.output.tail_au&#39;, &#39;rougher.calculation.sulfate_to_au_concentrate&#39;, &#39;rougher.calculation.floatbank10_sulfate_to_au_feed&#39;, &#39;rougher.calculation.floatbank11_sulfate_to_au_feed&#39;, &#39;rougher.calculation.au_pb_ratio&#39;, &#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.concentrate_au&#39;, &#39;rougher.output.recovery&#39;, &#39;rougher.output.tail_ag&#39;, &#39;rougher.output.tail_pb&#39;, &#39;rougher.output.tail_sol&#39;, &#39;rougher.output.tail_au&#39;, &#39;secondary_cleaner.output.tail_ag&#39;, &#39;secondary_cleaner.output.tail_pb&#39;, &#39;secondary_cleaner.output.tail_sol&#39;, &#39;secondary_cleaner.output.tail_au&#39;] . . Nearly all this values that are not in the test table can be predicted from existing values, so they are targets. Test table consist of features values. . Deal with NaN values . target_columns = [&#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;] features_columns = [x for x in train.columns if x in test.columns] features_columns = features_columns[1:] for i in features_columns: i.replace(&#39; n &#39;, &#39;&#39;) target_features_colums = target_columns + features_columns . print(&quot;We will lost {0:.2f}% of test data&quot;.format((len(train) - len(train.dropna(subset=target_features_colums))) / len(train))) . We will lost 0.25% of test data . print(&quot;We will lost {0:.2f}% of test data&quot;.format((len(test) - len(test.dropna())) / len(test))) . We will lost 0.08% of test data . train = train.dropna(subset=features_columns) train = train.dropna(subset=target_columns) test = test.dropna() . EDA . Dependence between concentrations of metals (Au, Ag, Pb) and purification stage . concent_au = full[[&#39;rougher.output.concentrate_au&#39;, &#39;primary_cleaner.output.concentrate_au&#39;, &#39;final.output.concentrate_au&#39;]] concent_ag = full[[&#39;rougher.output.concentrate_ag&#39;, &#39;primary_cleaner.output.concentrate_ag&#39;, &#39;final.output.concentrate_ag&#39;]] concent_pd = full[[&#39;rougher.output.concentrate_pb&#39;, &#39;primary_cleaner.output.concentrate_pb&#39;, &#39;final.output.concentrate_pb&#39;]] . plt.hist(concent_au.iloc[:, 0], bins=20, density=True, label=&#39;rough&#39;, histtype = &#39;step&#39;) plt.hist(concent_au.iloc[:, 1], bins=20, density=True, label=&#39;primary&#39;, histtype = &#39;step&#39;) plt.hist(concent_au.iloc[:, 2], bins=20, density=True, label=&#39;final&#39;, histtype = &#39;step&#39;) plt.legend() plt.show() . plt.hist(concent_ag.iloc[:, 0], bins=20, density=True, label=&#39;rough&#39;, histtype = &#39;step&#39;) plt.hist(concent_ag.iloc[:, 1], bins=20, density=True, label=&#39;primary&#39;, histtype = &#39;step&#39;) plt.hist(concent_ag.iloc[:, 2], bins=20, density=True, label=&#39;final&#39;, histtype = &#39;step&#39;) plt.legend() plt.show() . plt.hist(concent_pd.iloc[:, 0], bins=20, density=True, label=&#39;rough&#39;, histtype = &#39;step&#39;) plt.hist(concent_pd.iloc[:, 1], bins=20, density=True, label=&#39;primary&#39;, histtype = &#39;step&#39;) plt.hist(concent_pd.iloc[:, 2], bins=20, density=True, label=&#39;final&#39;, histtype = &#39;step&#39;) plt.legend() plt.show() . By the histograms we can say that concentration of gold is increasing on each cleaner step and it&#39;s expected. Obviouse all purification processes filter a lot off non-related elements, like Silver. Concentrate of Plumbum is encrease as well, it&#39;s interesting why. . Compare the feed particle size distributions in the train and in the test sets . feed_columns = [&#39;primary_cleaner.input.feed_size&#39;, &#39;rougher.input.feed_size&#39;] train_feed = train[feed_columns] test_feed = test[feed_columns] . plt.hist(train_feed.iloc[:, 0], bins=20, density=True, label=&#39;train&#39;, histtype = &#39;step&#39;) plt.hist(test_feed.iloc[:, 0], bins=20, density=True, label=&#39;test&#39;, histtype = &#39;step&#39;) plt.legend() plt.show() . plt.hist(train_feed.iloc[:, 1], bins=20, density=True, label=&#39;train&#39;, histtype = &#39;step&#39;) plt.hist(test_feed.iloc[:, 1], bins=20, density=True, label=&#39;test&#39;, histtype = &#39;step&#39;) plt.legend() plt.show() . There is almost no difference and we can consider these distribution the same. . Consider the total concentrations of all substances at different stages . raw_feed = [&#39;rougher.input.feed_ag&#39;, &#39;rougher.input.feed_pb&#39;, &#39;rougher.input.feed_sol&#39;, &#39;rougher.input.feed_au&#39;] rougher_concentrate = [&#39;rougher.output.concentrate_ag&#39;, &#39;rougher.output.concentrate_pb&#39;, &#39;rougher.output.concentrate_sol&#39;, &#39;rougher.output.concentrate_au&#39;] final_concentrate = [&#39;final.output.concentrate_ag&#39;, &#39;final.output.concentrate_pb&#39;, &#39;final.output.concentrate_sol&#39;, &#39;final.output.concentrate_au&#39;] . full[raw_feed].sum(1).hist(bins=50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5425f91950&gt; . full[rougher_concentrate].sum(1).hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5425e6b810&gt; . full[final_concentrate].sum(1).hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5425fe9ed0&gt; . train = train.loc[train[raw_feed].sum(1) &gt; 1] test = test.loc[test[raw_feed].sum(1) &gt; 1] train = train.loc[train[rougher_concentrate].sum(1) &gt; 1] train = train.loc[train[final_concentrate].sum(1) &gt; 1] . Train the model . features_r_columns = [x for x in features_columns if &#39;rougher.input&#39; in x] features_f_columns = [x for x in features_columns if &#39;primary&#39; in x] . features_r = train[features_r_columns] features_f = train[features_f_columns] target_r = train[target_columns[0]] target_f = train[target_columns[1]] . def smape(target, prediction): s = sum(np.abs(prediction - target) / ((np.abs(target) + np.abs(prediction)) / 2)) return s * 100 / len(target) . my_scorer = make_scorer(smape, greater_is_better=False) . model = LinearRegression() print(&quot;LinearRegression:&quot;, cross_val_score(model, features_r, target_r, cv=3, scoring=my_scorer).mean()) . LinearRegression: -7.73921166706288 . for i in range(1,10): model = DecisionTreeRegressor(max_depth = i, random_state=12345) print(&quot;DecisionTreeRegressor:&quot;, cross_val_score(model, features_r, target_r, cv=3, scoring=my_scorer).mean()) . DecisionTreeRegressor: -8.309019116513007 DecisionTreeRegressor: -8.414436821521468 DecisionTreeRegressor: -8.694769910780858 DecisionTreeRegressor: -9.063014057160933 DecisionTreeRegressor: -9.06968621135918 DecisionTreeRegressor: -9.205584731735364 DecisionTreeRegressor: -9.940410399198106 DecisionTreeRegressor: -10.229794791833923 DecisionTreeRegressor: -10.344914895926372 . for i in range(20,50,10): model = RandomForestRegressor(n_estimators=i, max_depth=3, random_state=12345) print(&quot;RandomForestRegressor with&quot;, i, &quot;estimators:&quot;, cross_val_score(model, features_r, target_r, cv=3, scoring=my_scorer).mean()) . RandomForestRegressor with 20 estimators: -8.586403536006658 RandomForestRegressor with 30 estimators: -8.596271567881844 RandomForestRegressor with 40 estimators: -8.584381133310744 . model = LinearRegression() print(&quot;LinearRegression:&quot;, cross_val_score(model, features_f, target_f, cv=3, scoring=my_scorer).mean()) . LinearRegression: -9.838455498334676 . for i in range(1,10): model = DecisionTreeRegressor(max_depth = i, random_state=12345) print(&quot;DecisionTreeRegressor:&quot;, i, &quot;max_depth:&quot;, cross_val_score(model, features_f, target_f, cv=3, scoring=my_scorer).mean()) . DecisionTreeRegressor: 1 max_depth: -9.547204412179097 DecisionTreeRegressor: 2 max_depth: -10.022658944341572 DecisionTreeRegressor: 3 max_depth: -10.349518009534277 DecisionTreeRegressor: 4 max_depth: -10.388885177403747 DecisionTreeRegressor: 5 max_depth: -10.738721462654391 DecisionTreeRegressor: 6 max_depth: -11.254489636909733 DecisionTreeRegressor: 7 max_depth: -11.172120146436209 DecisionTreeRegressor: 8 max_depth: -11.796373327634669 DecisionTreeRegressor: 9 max_depth: -12.106619581483434 . for i in range(20,50,10): model = RandomForestRegressor(n_estimators=20, max_depth=1, random_state=12345) print(&quot;RandomForestRegressor with&quot;, i, &quot;estimators:&quot;, cross_val_score(model, features_f, target_f, cv=3, scoring=my_scorer).mean()) . RandomForestRegressor with 20 estimators: -9.3756776856768 RandomForestRegressor with 30 estimators: -9.3756776856768 RandomForestRegressor with 40 estimators: -9.3756776856768 . The most better model for rougher target is LinearRegression, for final recovery target is RandomForestRegressor with max_depth=1. Will test them . Test the model . targets_all = full[[&#39;date&#39;, &#39;rougher.output.recovery&#39;, &#39;final.output.recovery&#39;]] test_new = test.set_index(&#39;date&#39;).join(targets_all.set_index(&#39;date&#39;), on=&#39;date&#39;, how=&#39;left&#39;) . test_new = test_new.dropna() . dummy_regr = DummyRegressor(strategy=&quot;mean&quot;) dummy_regr.fit(features_r, target_r) dummy_predict_r = dummy_regr.predict(test_new[features_r_columns]) dummy_regr = DummyRegressor(strategy=&quot;mean&quot;) dummy_regr.fit(features_f, target_f) dummy_predict_f = dummy_regr.predict(test_new[features_f_columns]) . final_smape_dummy = 0.25*smape(test_new[&#39;rougher.output.recovery&#39;], dummy_predict_r) + 0.75*smape(test_new[&#39;final.output.recovery&#39;], dummy_predict_f) final_smape_dummy . model = LinearRegression() model.fit(features_r, target_r) predict_r = model.predict(test_new[features_r_columns]) rough_smape = smape(test_new[&#39;rougher.output.recovery&#39;], predict_r) rough_smape . model = RandomForestRegressor(n_estimators=20, max_depth=1, random_state=12345) model.fit(features_f, target_f) predict_f = model.predict(test_new[features_f_columns]) final_smape = smape(test_new[&#39;final.output.recovery&#39;], predict_f) final_smape . final_smape = 0.25*smape(test_new[&#39;rougher.output.recovery&#39;], predict_r) + 0.75*smape(test_new[&#39;final.output.recovery&#39;], predict_f) final_smape . Conclusion . The Final sMAPE of the LinearRegression and RandomForestRegressor is better than final sMAPE of DummyRegression created by mean, but not too much. We can use our model, but the dummy regressor show, that it can be omitted. .",
            "url": "https://pluzharovakp.github.io/ds_projects/2020/08/17/Gold_Mining-Project.html",
            "relUrl": "/2020/08/17/Gold_Mining-Project.html",
            "date": " â€¢ Aug 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! Iâ€™m glad to see you on my Data Science Projects site. . My name is Kristina Pluzharova. Iâ€™m currently working as QA Engineer and I have 4+ years of experience in manual and automation testing. Being a Data scientist was always in my carrier plans, so I finished DS course by Yandex.Practicum and now take projects to expand my knowledge and to get more experience. . Thank you for your attention! .",
          "url": "https://pluzharovakp.github.io/ds_projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://pluzharovakp.github.io/ds_projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}